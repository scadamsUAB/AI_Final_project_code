{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"GeneticAnnealingGAN.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNH6ftIQMzgu52UUbtKMRfO"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"vfnIzGjMk_ar"},"source":["## CODE BASE FOR THE MODEL REFERENCED THE FOLLOWING SOURCE: \n","## https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_2_Keras_gan.ipynb\n","## Reference file was for face generation, but was modified for dog / cat data. \n","\n","## Data source was https://www.kaggle.com/shaunthesheep/microsoft-catsvsdogs-dataset  for balanced training.  \n","\n","## Imbalanced data source removed 25% or dog images\n","\n","import tensorflow as tf \n","from tensorflow.keras.layers import Input, Reshape, Dropout, Dense \n","from tensorflow.keras.layers import Flatten, BatchNormalization\n","from tensorflow.keras.layers import Activation, ZeroPadding2D\n","from tensorflow.keras.layers import LeakyReLU\n","from tensorflow.keras.layers import UpSampling2D, Conv2D\n","from tensorflow.keras.models import Sequential, Model, load_model\n","from tensorflow.keras.optimizers import Adam, SGD, Nadam\n","import random\n","import numpy as np\n","from PIL import Image\n","from tqdm import tqdm\n","import os \n","import time\n","import matplotlib.pyplot as plt\n","### In order to spped up training.  Use the notebook as part of Google Colab with GPU Runtimes enabled\n","\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s2S0iMRpgMkT"},"source":[""]},{"cell_type":"code","metadata":{"id":"iNdrfMxyl5Fk"},"source":["### Check to see if the notebook is running on Google Colab or locally. \n","### If running on Google Colab, you will need to connect your Google Drive\n","### in order to access training data\n","try:\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","    COLAB = True\n","    print(\"Note: using Google CoLab\")\n","    %tensorflow_version 2.x\n","except:\n","    print(\"Note: not using Google CoLab\")\n","    COLAB = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4I1_4Tl1l9md"},"source":["## REF: https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_2_Keras_gan.ipynb\n","\n","# Generation resolution - Must be square \n","# Training data is also scaled to this.\n","# Note GENERATE_RES 4 or higher will blow Google CoLab's memory\n","\n","GENERATE_RES = 2 # Generation resolution factor \n","# (1=32, 2=64, 3=96, 4=128, etc.)\n","GENERATE_SQUARE = 32 * GENERATE_RES # rows/cols (should be square)\n","IMAGE_CHANNELS = 3\n","\n","### These constants are used to support the output of sample images in a grid foramt\n","# Preview image \n","PREVIEW_ROWS = 4\n","PREVIEW_COLS = 7\n","PREVIEW_MARGIN = 16\n","\n","# Size vector to generate images from (sandard value in most research is 100)\n","SEED_SIZE = 100\n","\n","\n","### Modify these values to fine tune training and provide a path to training data if using Google Colab\n","DATA_PATH = '/content/drive/MyDrive/ColabNotebooks/Pet/'\n","EPOCHS = 1000\n","BATCH_SIZE = 16\n","BUFFER_SIZE = 60000\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aC8xJK0JEbeq"},"source":["## REF ## https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_2_Keras_gan.ipynb\n","### To save time on subsequent runs, save the training images in .npy file.  This prevents the files from being read each time the process is re-ran\n","training_binary_path = os.path.join(DATA_PATH,  f'training_data_{GENERATE_SQUARE}_{GENERATE_SQUARE}.npy')\n","print(f\"Looking for file: {training_binary_path}\")\n","\n","### If the .npy file is not found load images from image path and save a new .npy file\n","if not os.path.isfile(training_binary_path):\n","  print(\"Loading training images...\")\n","  try:\n","    training_data = []\n","    faces_path = os.path.join(DATA_PATH,'PetImbalance')\n","    print(\"PATH:\", faces_path)\n","    for filename in tqdm(os.listdir(faces_path)):\n","        path = os.path.join(faces_path,filename)\n","        ### Some images may be black and white instead of color.  \n","        ### Corrupt images may also be present. \n","        ### Use try / except to remove those files when loading\n","        try:\n","          image = Image.open(path).convert('RGB').resize((GENERATE_SQUARE,GENERATE_SQUARE),Image.ANTIALIAS)\n","          training_data.append(np.asarray(image))\n","        except: \n","          print(\"bad file:\", path, filename)\n","  except:\n","    print(\"Exception \",path )\n","  training_data = np.reshape(training_data,(-1,GENERATE_SQUARE, GENERATE_SQUARE,IMAGE_CHANNELS))\n","  training_data = training_data.astype(np.float32)\n","  training_data = training_data / 127.5 - 1.\n","\n","\n","  print(\"Saving training image binary...\")\n","  np.save(training_binary_path,training_data)\n","else:\n","  print(\"Loading previous training binary...\")\n","  training_data = np.load(training_binary_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DY12VV1SgHy4"},"source":["# Batch and shuffle the data\n","train_dataset = tf.data.Dataset.from_tensor_slices(training_data).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GhQNeqJdze46"},"source":["## REF: ## https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_2_Keras_gan.ipynb\n","def build_generator(seed_size, channels):\n","    model = Sequential()\n","\n","    model.add(Dense(4*4*256,activation=\"relu\",input_dim=seed_size))\n","    model.add(Reshape((4,4,256)))\n","\n","    model.add(UpSampling2D())\n","    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n","    model.add(BatchNormalization(momentum=0.8))\n","    model.add(Activation(\"relu\"))\n","\n","    model.add(UpSampling2D())\n","    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n","    model.add(BatchNormalization(momentum=0.8))\n","    model.add(Activation(\"relu\"))\n","   \n","    # Output resolution, additional upsampling\n","    model.add(UpSampling2D())\n","    model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n","    model.add(BatchNormalization(momentum=0.8))\n","    model.add(Activation(\"relu\"))\n","\n","    if GENERATE_RES>1:\n","      model.add(UpSampling2D(size=(GENERATE_RES,GENERATE_RES)))\n","      model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n","      model.add(BatchNormalization(momentum=0.8))\n","      model.add(Activation(\"relu\"))\n","\n","    # Final CNN layer\n","    model.add(Conv2D(channels,kernel_size=3,padding=\"same\"))\n","    model.add(Activation(\"tanh\"))\n","\n","    return model\n","\n","\n","def build_discriminator(image_shape):\n","    model = Sequential()\n","\n","    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=image_shape, \n","                     padding=\"same\"))\n","    model.add(LeakyReLU(alpha=0.2))\n","\n","    model.add(Dropout(0.25))\n","    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n","    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n","    model.add(BatchNormalization(momentum=0.8))\n","    model.add(LeakyReLU(alpha=0.2))\n","\n","    model.add(Dropout(0.25))\n","    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n","    model.add(BatchNormalization(momentum=0.8))\n","    model.add(LeakyReLU(alpha=0.2))\n","\n","    model.add(Dropout(0.25))\n","    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n","    model.add(BatchNormalization(momentum=0.8))\n","    model.add(LeakyReLU(alpha=0.2))\n","\n","    model.add(Dropout(0.25))\n","    model.add(Conv2D(512, kernel_size=3, strides=1, padding=\"same\"))\n","    model.add(BatchNormalization(momentum=0.8))\n","    model.add(LeakyReLU(alpha=0.2))\n","\n","    model.add(Dropout(0.25))\n","    model.add(Flatten())\n","    model.add(Dense(1, activation='sigmoid'))\n","\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I8CB8-zBz3p8"},"source":["## REF: ## https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_2_Keras_gan.ipynb\n","def save_images(cnt,noise,file_name_prefix=\"default_\"):\n","  ### Create a grid to output multiple generated images at the same time. \n","  image_array = np.full(( PREVIEW_MARGIN + (PREVIEW_ROWS * (GENERATE_SQUARE+PREVIEW_MARGIN)),  PREVIEW_MARGIN + (PREVIEW_COLS * (GENERATE_SQUARE+PREVIEW_MARGIN)), IMAGE_CHANNELS),  255, dtype=np.uint8)\n","  \n","  generated_images = generator.predict(noise)\n","  generated_images = 0.5 * generated_images + 0.5\n","\n","  image_count = 0\n","  for row in range(PREVIEW_ROWS):\n","      for col in range(PREVIEW_COLS):\n","        r = row * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n","        c = col * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n","        image_array[r:r+GENERATE_SQUARE,c:c+GENERATE_SQUARE] \\\n","            = generated_images[image_count] * 255\n","        image_count += 1\n","\n","          \n","  output_path = os.path.join(DATA_PATH,'output')\n","  if not os.path.exists(output_path):\n","    os.makedirs(output_path)\n","  \n","  filename = os.path.join(output_path,f\"{file_name_prefix}train-{cnt}.png\")\n","  im = Image.fromarray(image_array)\n","  im.save(filename)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DUxdjKx_z-RT"},"source":["generator = build_generator(SEED_SIZE, IMAGE_CHANNELS)\n","#generator.save(os.path.join(DATA_PATH,\"gen1.h5\"))\n","\n","#noise = tf.random.normal([1, SEED_SIZE])\n","image_shape = (GENERATE_SQUARE,GENERATE_SQUARE,IMAGE_CHANNELS)\n","\n","discriminator = build_discriminator(image_shape)\n","#discriminator.save(os.path.join(DATA_PATH,\"disc1.h5\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V16a3qPD0NkN"},"source":["# This method returns a helper function to compute cross entropy loss\n","cross_entropy = tf.keras.losses.BinaryCrossentropy()\n","def discriminator_loss(real_output, fake_output):\n","    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n","    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n","    total_loss = real_loss + fake_loss\n","    return total_loss\n","\n","def generator_loss(fake_output):\n","    return cross_entropy(tf.ones_like(fake_output), fake_output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_BApstto3kNp"},"source":["## MODIFIED FROM https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_2_Keras_gan.ipynb\n","## UPDATED TO TAKE IN PASSED PARAMETERS FOR LOADED MODELS\n","def train_step(images,generator,discriminator,gen_opt, disc_opt):\n","\n","  seed = tf.random.normal([BATCH_SIZE, SEED_SIZE])\n","\n","  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","    generated_images = generator(seed, training=True)\n","\n","    real_output = discriminator(images, training=True)\n","    fake_output = discriminator(generated_images, training=True)\n","    \n","    gen_loss = generator_loss(fake_output)\n","    disc_loss = discriminator_loss(real_output, fake_output)\n","    \n","    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n","    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n","\n","    gen_opt.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n","    disc_opt.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n","  return gen_loss,disc_loss, generator, discriminator, gen_opt,disc_opt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ib992cBRAhH7"},"source":["## MODIFIED FROM https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_2_Keras_gan.ipynb\n","## UPDATED TO TAKE IN PASSED PARAMETERS FOR LOADED MODELS\n","def train(dataset, gen, disc, gen_opt,disc_opt,e,file_name_prefix = \"default\"):\n","  fixed_seed = np.random.normal(0, 1, (PREVIEW_ROWS * PREVIEW_COLS, SEED_SIZE))\n","  gen_loss_list = [] \n","  disc_loss_list = []\n","\n","  for image_batch in dataset:\n","      t = train_step(image_batch,gen,disc,gen_opt, disc_opt)\n","      gen_loss_list.append(t[0])\n","      disc_loss_list.append(t[1])\n","      gen=t[2]\n","      disc=t[3]\n","      gen_opt = t[4]\n","      disc_opt = t[5]\n","\n","  g_loss = sum(gen_loss_list) / len(gen_loss_list)\n","  d_loss = sum(disc_loss_list) / len(disc_loss_list)\n","\n","  save_images(e,fixed_seed,file_name_prefix)\n","\n","\n","  return g_loss, gen, disc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"uHfLzQZ13ukL"},"source":["  Temp = EPOCHS *.75\n","  dec = Temp / EPOCHS\n","  print(\"TEMP  DEC\", Temp, dec)\n","\n","  for e in range(EPOCHS):\n","\n","    generator_optimizer = tf.keras.optimizers.Adam(1.5e-6,0.5)\n","    discriminator_optimizer = tf.keras.optimizers.Adam(1.5e-6,0.5)\n","    try:\n","      print(\"load and compile\")\n","      gen = load_model(os.path.join(DATA_PATH,\"gen1.h5\"),)\n","      gen.compile(optimizer=generator_optimizer, loss = cross_entropy)\n","      disc =load_model(os.path.join(DATA_PATH,\"disc1.h5\"))\n","      disc.compile(optimizer=discriminator_optimizer, loss = cross_entropy)\n","    except:\n","      gen = generator\n","      disc = discriminator\n","      ## If no model currently exists in a save state, create an initial model by performing a single train operation\n","      l1, gen2, disc2 =train(train_dataset, gen, disc, generator_optimizer,discriminator_optimizer,e,\"1\")\n","      print(f\"EPOCH {e}  LOSS: {l1}\")\n","      gen2.save(os.path.join(DATA_PATH,\"gen1.h5\"))\n","      disc2.save(os.path.join(DATA_PATH,\"disc1.h5\"))\n","     \n","    \n","    ### Execute each of the training iterations using the loaded model as a baseline\n","    ### Save the result into a new temporary model\n","\n","    l1, gen2, disc2 =train(train_dataset, gen, disc, generator_optimizer,discriminator_optimizer,e,\"1\")\n","    print(f\"EPOCH {e}  LOSS: {l1}\")\n","    try:\n","      gen2.save(os.path.join(DATA_PATH,\"gen2.h5\"))\n","      disc2.save(os.path.join(DATA_PATH,\"disc2.h5\"))\n","    except:\n","      pass\n","    min = l1\n","    min_id = \"l1\"\n","\n","    ### Update Optimizer for second instance\n","    generator_optimizer = tf.keras.optimizers.SGD(1.5e-3,0.005, False)\n","    discriminator_optimizer = tf.keras.optimizers.SGD(1.5e-3,0.005, False)\n","    l2, gen3, disc3 =train(train_dataset, gen, disc, generator_optimizer,discriminator_optimizer,e,\"2\") \n","    print(f\"EPOCH {e}  LOSS: {l2}\")\n","    try:\n","      gen3.save(os.path.join(DATA_PATH,\"gen2.h5\"))\n","      disc3.save(os.path.join(DATA_PATH,\"disc2.h5\"))\n","    except:\n","      pass\n","    if l2 < min:\n","      min = l2\n","      min_id = \"l2\"\n","\n","\n","    ### Update Optimizer for third instance\n","    generator_optimizer = tf.keras.optimizers.Nadam(1.5e-3,0.25)\n","    discriminator_optimizer = tf.keras.optimizers.Nadam(1.5e-3,0.25)\n","    l3, gen4, disc4 =train(train_dataset, gen, disc, generator_optimizer,discriminator_optimizer,e,\"3\") \n","    print(f\"EPOCH {e}  LOSS: {l3}\")\n","    try:\n","      gen4.save(os.path.join(DATA_PATH,\"gen4.h5\"))\n","    \n","      disc4.save(os.path.join(DATA_PATH,\"disc4.h5\"))\n","    except:\n","      pass\n","    if l3 < min:\n","      min = l3\n","      min_id = \"l3\"\n","\n","\n","    ### Decrase Temprature value\n","    ### Choose a random number between 0 and the number of EPOCHS\n","    ### If the random value is less then the Temp, allow for a random choice\n","    ### Otherwise pick the model with the lowest loss as the baseline for next Epoch\n","    Temp = Temp - dec\n","    rand = random.randint(0, EPOCHS)\n","  \n","    if rand < Temp:\n","      rand2 = random.randint(0,3)\n","      print(\"RAND 2\" , rand2)\n","      if rand2 ==0:\n","        print(\"Random choice : first option\")\n","        try:\n","          gen2.save(os.path.join(DATA_PATH,\"gen1.h5\"))\n","          disc2.save(os.path.join(DATA_PATH,\"disc1.h5\"))\n","        except:\n","          pass\n","      elif rand2 == 1:\n","        print(\"Random choice : second option\")\n","        try:\n","\n","          gen3.save(os.path.join(DATA_PATH,\"gen1.h5\"))\n","          disc3.save(os.path.join(DATA_PATH,\"disc1.h5\"))\n","        except:\n","          pass\n","      else:\n","        print(\"Random choice : third option \")\n","        try:\n","          gen4.save(os.path.join(DATA_PATH,\"gen1.h5\"))\n","          disc4.save(os.path.join(DATA_PATH,\"disc1.h5\"))\n","        except:\n","          pass\n","    else:\n","      if min_id ==\"l3\":\n","        print(\"Choosing best : third option\")\n","        try:\n","          gen4.save(os.path.join(DATA_PATH,\"gen1.h5\"))\n","          disc4.save(os.path.join(DATA_PATH,\"disc1.h5\"))\n","        except:\n","          pass\n","      elif min_id == \"l2\":\n","        print(\"Choosing best : second option\")\n","        try:\n","          gen3.save(os.path.join(DATA_PATH,\"gen1.h5\"))\n","          disc3.save(os.path.join(DATA_PATH,\"disc1.h5\"))\n","        except:\n","          pass\n","      else:\n","        print(\"Choosing best : first option\")\n","        \n","        try:\n","          gen2.save(os.path.join(DATA_PATH,\"gen1.h5\"))\n","          disc2.save(os.path.join(DATA_PATH,\"disc1.h5\"))\n","        except:\n","          pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6KulNdVa3vTs"},"source":["generator.save(os.path.join(DATA_PATH,\"final_generator.h5\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TC3XFsW79-FV"},"source":[""],"execution_count":null,"outputs":[]}]}